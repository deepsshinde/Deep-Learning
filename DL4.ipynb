{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a77b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use Autoencoder to implement anomaly detection. Build the model by using:\n",
    "a. Import required libraries\n",
    "b. Upload / access the dataset\n",
    "c. Encoder converts it into latent representation\n",
    "d. Decoder networks convert it back to the original input\n",
    "e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ec66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a: Import required libraries\n",
    "\n",
    "# pandas is used for data manipulation and analysis. It provides data structures like DataFrames, \n",
    "# which are useful for handling tabular data (like CSV files).\n",
    "import pandas as pd  \n",
    "\n",
    "# numpy is a library for numerical computing in Python. It provides support for large, multi-dimensional arrays \n",
    "# and matrices, and includes mathematical functions to operate on these arrays.\n",
    "import numpy as np  \n",
    "\n",
    "# matplotlib.pyplot is a plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "# It is commonly used for generating graphs, charts, and other visualizations.\n",
    "import matplotlib.pyplot as plt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8248fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Import StandardScaler to standardize the features (mean = 0, std = 1)\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "# Import Model to define a custom Keras model (functional API)\n",
    "from keras.models import Model  \n",
    "\n",
    "# Import Input to define the input layer and Dense for fully connected layers\n",
    "from keras.layers import Input, Dense  \n",
    "\n",
    "# Import regularizers to apply regularization (L1, L2) to the model\n",
    "from keras import regularizers  \n",
    "\n",
    "# Import EarlyStopping to halt training if the validation loss doesn't improve (prevents overfitting)\n",
    "from keras.callbacks import EarlyStopping  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a135ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credit card fraud detection dataset from a CSV file\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Standardize the 'Amount' column to have a mean of 0 and standard deviation of 1 (for better model performance)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Drop the 'Time' column as it is not useful for the anomaly detection task\n",
    "data = data.drop(['Time'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cb423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Only use non-fraudulent transactions (Class == 0) for training the Autoencoder\n",
    "# Drop the 'Class' column from the training set as it is not used in the input for anomaly detection\n",
    "X_train = X_train[X_train.Class == 0].drop(['Class'], axis=1).values\n",
    "\n",
    "# Separate the 'Class' column (target variable) from the test set for evaluation\n",
    "y_test = X_test['Class']\n",
    "\n",
    "# Drop the 'Class' column from the test set to leave only features for prediction\n",
    "X_test = X_test.drop(['Class'], axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abe8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the input dimension based on the number of features in the training data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define the encoding dimension (size of the latent space representation)\n",
    "encoding_dim = 14\n",
    "\n",
    "# Input layer: Defines the shape of the input (based on the number of features in the dataset)\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder: First fully connected layer with 'tanh' activation and L1 regularization\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "\n",
    "# Encoder: Second fully connected layer with 'relu' activation to reduce the encoding dimension\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "# Decoder: First fully connected layer with 'tanh' activation to reconstruct the data\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "\n",
    "# Decoder: Final fully connected layer to output the reconstruction with the same shape as the input\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "# Autoencoder model: Defines the complete model with input and output layers\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67701d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "7108/7108 [==============================] - 13s 2ms/step - loss: 0.8073 - val_loss: 0.7798\n",
      "Epoch 2/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7399 - val_loss: 0.7610\n",
      "Epoch 3/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7275 - val_loss: 0.7525\n",
      "Epoch 4/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7210 - val_loss: 0.7485\n",
      "Epoch 5/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7181 - val_loss: 0.7480\n",
      "Epoch 6/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7162 - val_loss: 0.7453\n",
      "Epoch 7/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7143 - val_loss: 0.7425\n",
      "Epoch 8/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7118 - val_loss: 0.7406\n",
      "Epoch 9/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7099 - val_loss: 0.7401\n",
      "Epoch 10/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7089 - val_loss: 0.7395\n",
      "Epoch 11/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7079 - val_loss: 0.7390\n",
      "Epoch 12/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7059 - val_loss: 0.7365\n",
      "Epoch 13/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7041 - val_loss: 0.7366\n",
      "Epoch 14/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7033 - val_loss: 0.7388\n",
      "Epoch 15/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7030 - val_loss: 0.7351\n",
      "Epoch 16/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7028 - val_loss: 0.7436\n",
      "Epoch 17/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7028 - val_loss: 0.7364\n",
      "Epoch 18/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7023 - val_loss: 0.7358\n",
      "Epoch 19/20\n",
      "7108/7108 [==============================] - 12s 2ms/step - loss: 0.7021 - val_loss: 0.7384\n",
      "Epoch 20/20\n",
      "7108/7108 [==============================] - 13s 2ms/step - loss: 0.7019 - val_loss: 0.7356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17a1374cb80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the autoencoder model using the Adam optimizer and mean squared error loss function\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define early stopping to halt training if validation loss does not improve for 10 consecutive epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the autoencoder model with the training data, validate on test data, and apply early stopping\n",
    "autoencoder.fit(X_train, X_train, epochs=20, batch_size=32, validation_data=(X_test, X_test), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a66af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781/1781 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the trained autoencoder model on the test data\n",
    "predictions = autoencoder.predict(X_test)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) between the original and predicted data for each test sample\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection (samples with MSE greater than the threshold are considered anomalies)\n",
    "threshold = 50  # Set based on analysis\n",
    "\n",
    "# Classify anomalies: If MSE is greater than the threshold, mark as anomaly (1), else normal (0)\n",
    "y_pred = [1 if e > threshold else 0 for e in mse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff70d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics from sklearn to evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b051e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983146659176293\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20727d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.25510204081632654\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\", recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f073d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5208333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bebac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[56841    23]\n",
      " [   73    25]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the key theory concepts associated with the Autoencoder-based anomaly detection practical that an external examiner might ask about:\n",
    "\n",
    "1. **Autoencoder Architecture**:\n",
    "   - **Encoder**: Compresses the input data into a lower-dimensional latent representation.\n",
    "   - **Decoder**: Reconstructs the input data from the latent representation.\n",
    "   - **Symmetric Architecture**: Often, the encoder and decoder have symmetric layers.\n",
    "\n",
    "2. **Loss Function**:\n",
    "   - **Mean Squared Error (MSE)**: Commonly used in Autoencoders for reconstruction tasks, measures the difference between original and reconstructed data.\n",
    "   - **Loss Function for Anomaly Detection**: The reconstruction error (MSE) is higher for anomalies, as the Autoencoder fails to accurately reconstruct abnormal data.\n",
    "\n",
    "3. **Anomaly Detection**:\n",
    "   - Anomalies are detected by thresholding the reconstruction error (MSE) — samples with high reconstruction errors are considered anomalies.\n",
    "   - **Threshold**: A pre-set threshold (like `50` in your code) determines if the reconstruction error is high enough to classify a sample as an anomaly.\n",
    "\n",
    "4. **Activation Functions**:\n",
    "   - **ReLU (Rectified Linear Unit)**: Used in the encoder and decoder for introducing non-linearity.\n",
    "   - **Tanh**: Used in the encoder to map values between -1 and 1, often used for generating latent representations.\n",
    "\n",
    "5. **Regularization**:\n",
    "   - **L1 Regularization**: Adds a penalty to the model's complexity by forcing some weights to be exactly zero, helping to prevent overfitting.\n",
    "   - **Activity Regularizer**: Penalizes large activation values, often used in Autoencoders for improving generalization.\n",
    "\n",
    "6. **Training Techniques**:\n",
    "   - **Early Stopping**: A technique used to stop training when the validation loss stops improving, which helps prevent overfitting.\n",
    "   - **Adam Optimizer**: Adaptive learning rate optimizer widely used in deep learning tasks.\n",
    "   \n",
    "7. **Model Evaluation**:\n",
    "   - **Accuracy**: Measures the percentage of correct predictions.\n",
    "   - **Recall**: Focuses on detecting anomalies (true positives), measuring the ability to identify all actual anomalies.\n",
    "   - **Precision**: Measures the correctness of predicted anomalies.\n",
    "   - **Confusion Matrix**: Provides insight into true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "8. **Dimensionality Reduction**:\n",
    "   - Autoencoders can be viewed as a form of unsupervised **dimensionality reduction** that compresses the data into a smaller representation.\n",
    "\n",
    "These concepts are essential for understanding the functioning of Autoencoders in anomaly detection and can help explain the workings and rationale behind the model in your practical.\n",
    "\n",
    "Here are a few more minor points that could be relevant for an external examiner to ask:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Standardization**: Why is it important to standardize features (e.g., 'Amount' in the dataset)? Standardization ensures that all features have the same scale, improving model training and performance.\n",
    "   - **Handling Imbalanced Data**: Since fraud cases (anomalies) are typically rare, this can affect model performance. Although not directly addressed in your practical, this is a common challenge in anomaly detection tasks.\n",
    "\n",
    "2. **Latent Space**:\n",
    "   - The **latent space** is the compressed representation of the input data in the Autoencoder. The size of this space (e.g., 14 dimensions) controls the model's ability to capture underlying patterns.\n",
    "   - **Dimensionality Choice**: The choice of the latent space dimension (encoding_dim) is crucial — too small might lose important features, too large might not generalize well.\n",
    "\n",
    "3. **Overfitting and Underfitting**:\n",
    "   - **Overfitting** occurs when the model learns to memorize the training data, including noise, which can reduce its ability to generalize to new data.\n",
    "   - **Underfitting** happens when the model is too simple to capture the underlying patterns, leading to poor performance.\n",
    "\n",
    "4. **Reconstruction Error**:\n",
    "   - An **anomaly** is detected when the reconstruction error is large because the Autoencoder is trained primarily on normal data. When it encounters abnormal data, it fails to reconstruct it well.\n",
    "   - **Threshold for Anomaly Detection**: The threshold value for MSE is critical — choosing a wrong value might lead to too many false positives or false negatives.\n",
    "\n",
    "5. **Activation Functions (continued)**:\n",
    "   - **Tanh**: Has a range between -1 and 1, making it suitable for encoding and decoding, where the output values should be normalized or centered around zero.\n",
    "   - **ReLU**: Converts negative values to zero but keeps positive values intact, making it effective for non-linear tasks like anomaly detection.\n",
    "\n",
    "6. **Model Evaluation in Anomaly Detection**:\n",
    "   - **Precision vs Recall**: In fraud detection, recall is often more important than precision because detecting all fraud cases is critical, even if some normal cases are flagged as anomalies (false positives).\n",
    "   - **False Positive Rate**: An examiner might ask about the false positive rate, especially in fraud detection. A higher threshold for MSE will reduce false positives but may increase false negatives.\n",
    "\n",
    "7. **Training Time and Complexity**:\n",
    "   - **Epochs**: The number of epochs defines how many times the model sees the entire dataset. Too many epochs may cause overfitting, while too few may cause underfitting.\n",
    "   - **Batch Size**: Determines how many samples are used to calculate the gradient update at each step. Larger batch sizes can speed up training but require more memory.\n",
    "\n",
    "8. **Anomaly Detection Applications**:\n",
    "   - **Real-world use cases**: Autoencoders are widely used in **fraud detection**, **network intrusion detection**, **machine health monitoring**, and other areas where anomalies in data need to be identified. \n",
    "\n",
    "These points add more depth to the concepts and help an examiner understand your knowledge beyond just the practical implementation.\n",
    "\n",
    "Here are a few additional minor points that could be useful:\n",
    "\n",
    "1. **Autoencoder Variants**:\n",
    "   - **Denoising Autoencoders**: This variant adds noise to the input data and trains the model to reconstruct the clean data. It helps the model generalize better and is often used when noise is present in the dataset.\n",
    "   - **Variational Autoencoders (VAE)**: VAEs introduce a probabilistic approach and generate a distribution of possible latent variables, often used in generative tasks, but not typically in anomaly detection.\n",
    "\n",
    "2. **Role of Anomalies in the Dataset**:\n",
    "   - **Anomaly Definition**: Anomalies (or outliers) are rare events or observations that deviate significantly from the general pattern of the data. The goal of anomaly detection is to identify these outliers.\n",
    "   - **One-Class Classification**: Autoencoders are often used in **one-class classification** for anomaly detection, where the model is only trained on normal data, and anomalies are identified by high reconstruction errors.\n",
    "\n",
    "3. **Reconstruction vs Prediction**:\n",
    "   - In an Autoencoder, the goal is not to predict a label (as in traditional supervised learning) but to reconstruct the input. This is a key distinction between unsupervised learning (Autoencoder) and supervised learning (e.g., classification tasks).\n",
    "\n",
    "4. **Optimizers**:\n",
    "   - **Adam Optimizer**: Combines the benefits of two other extensions of stochastic gradient descent, namely **Momentum** and **RMSprop**, making it more efficient in practice for a variety of tasks.\n",
    "   - **Learning Rate**: The learning rate is a key hyperparameter that controls how much the weights are updated in each training step. Too high a learning rate can cause the model to converge too quickly, possibly missing the optimal solution; too low can make the training very slow.\n",
    "\n",
    "5. **Model Interpretability**:\n",
    "   - **Latent Space Visualization**: In some cases, it may be useful to visualize the latent space to understand how the model has compressed the data. This can help in understanding what the model has learned, especially when dealing with complex datasets.\n",
    "   - **Feature Importance**: While Autoencoders are typically not as interpretable as decision trees or linear models, analyzing the reconstruction error can give insights into which features are important for detecting anomalies.\n",
    "\n",
    "6. **Handling Missing Data**:\n",
    "   - **Imputation**: If the dataset has missing values, techniques such as imputation (e.g., mean or median imputation) might be used before training the model to ensure the data is complete.\n",
    "   - **Impact of Missing Data**: Missing data can affect the performance of the Autoencoder, as it may prevent the model from learning accurate representations of the input.\n",
    "\n",
    "7. **Evaluation Metrics for Anomaly Detection**:\n",
    "   - **F1-Score**: A balance between precision and recall, which is important in anomaly detection because it provides a combined measure of both false positives and false negatives.\n",
    "   - **ROC Curve & AUC**: The **Receiver Operating Characteristic (ROC)** curve plots the true positive rate against the false positive rate, and the **Area Under the Curve (AUC)** provides a single value that summarizes the model's ability to distinguish between normal and anomalous data.\n",
    "\n",
    "8. **Batch Normalization**:\n",
    "   - **Batch Normalization**: This technique is used in deep learning to normalize the inputs to each layer so that the model trains faster and more stably. It can sometimes help when training deeper Autoencoder models.\n",
    "\n",
    "9. **Overfitting in Anomaly Detection**:\n",
    "   - **Overfitting** in anomaly detection happens when the model is too complex and memorizes the normal data so well that it fails to generalize to new or unseen normal data. Early stopping, regularization, or simpler models are commonly used to mitigate overfitting.\n",
    "\n",
    "10. **Effect of Noise on Anomaly Detection**:\n",
    "    - **Noisy Data**: Noisy data can increase the reconstruction error for normal data, leading to false positives (misclassifying normal data as anomalies). Preprocessing steps like noise filtering or using Denoising Autoencoders can help mitigate this.\n",
    "\n",
    "11. **Sparse Autoencoders**:\n",
    "    - A **sparse Autoencoder** uses a sparsity constraint in the encoder, where only a small subset of the latent neurons are active at any given time. This leads to a more efficient representation, often used in tasks like feature selection.\n",
    "\n",
    "These additional points cover more advanced concepts, variations, and nuances that could enrich your understanding of Autoencoders and anomaly detection. They might help you answer more specific or detailed questions if asked by the examiner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
