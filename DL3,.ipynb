{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b34877",
   "metadata": {},
   "source": [
    "Build the Image classification model by dividing the model into following 4 stages:\n",
    "a. Loading and preprocessing the image data\n",
    "b. Defining the model’s architecture\n",
    "c. Training the model\n",
    "d. Estimating the model’s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0badddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TensorFlow, a deep learning library used for building and training models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing essential modules from Keras for building and structuring the neural network model\n",
    "from keras.models import Sequential           # Sequential model allows stacking layers in order\n",
    "from keras.layers import Dense, Conv2D,  Dropout, Flatten, MaxPooling2D\n",
    "# Importing matplotlib for visualizing data, such as training and validation loss/accuracy plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing NumPy for numerical operations and handling data arrays\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa9ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training data: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# A) Loading and Pre-processing the Image Data\n",
    "\n",
    "# Loading the MNIST dataset from TensorFlow. This dataset consists of 28x28 grayscale images of handwritten digits (0-9).\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Splitting the dataset into training and testing data.\n",
    "# (x_train, y_train) contains the images and labels for the training set.\n",
    "# (x_test, y_test) contains the images and labels for the testing set.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshaping the data to fit the input shape expected by the model.\n",
    "# The model expects input of shape (batch_size, height, width, channels).\n",
    "# The original shape of x_train is (number_of_samples, 28, 28) – 28x28 images.\n",
    "# We add the channel dimension, making it (number_of_samples, 28, 28, 1), \n",
    "# where '1' indicates that the images are grayscale.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Converting pixel values from integers to floats and normalizing the data.\n",
    "# MNIST images have pixel values between 0 and 255, and we scale them to a range of 0 to 1 \n",
    "# by dividing by 255. This helps the neural network learn more effectively.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Printing the shape of the training data to verify the changes.\n",
    "# The output will show the shape of the x_train dataset, which is now (num_samples, 28, 28, 1),\n",
    "# where num_samples is the number of training images (60,000 for MNIST).\n",
    "print(\"Shape of Training data:\", x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd8eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 28)        280       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 28)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4732)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 200)               946600    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 948890 (3.62 MB)\n",
      "Trainable params: 948890 (3.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# B) Defining the Model's Architecture\n",
    "\n",
    "# Initialize the Sequential model (stacking layers in order)\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer: 28 filters of size 3x3, 'relu' activation for feature extraction\n",
    "model.add(Conv2D(28, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "\n",
    "# Max pooling layer: 2x2 pooling to reduce spatial dimensions (downsampling)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer: Converts 2D output to 1D array for fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer: 200 neurons, 'relu' activation to interpret features\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Dropout layer: 30% dropout to prevent overfitting during training\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Dense output layer: 10 neurons (one per class), 'softmax' activation for class probabilities\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Display the model summary to view the architecture and parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ac1c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\ANACONDA\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1875/1875 [==============================] - 29s 14ms/step - loss: 0.1740 - accuracy: 0.9480\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0635 - accuracy: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x198cc5c48b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C) Training the Model\n",
    "\n",
    "# Compile the model with Adam optimizer, loss function, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 2 epochs using training data\n",
    "model.fit(x_train, y_train, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4b7232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9835\n",
      "Test Loss: 0.049\n",
      "Test Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "# D) Evaluating the Model\n",
    "\n",
    "# Evaluate the model on the test data and get the loss and accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss: {:.3f}\".format(test_loss))\n",
    "print(\"Test Accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2ff5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# E) Displaying an Image from the Dataset\n",
    "\n",
    "# Set the index of the image to display (0 corresponds to the first image in the training set)\n",
    "image_index = 0\n",
    "\n",
    "# Get the image at the specified index\n",
    "image = x_train[image_index]\n",
    "\n",
    "# Display the image using matplotlib, with grayscale colormap\n",
    "plt.imshow(np.squeeze(image), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842dc4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step\n",
      "Predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "# F) Predicting the Class of the Image\n",
    "\n",
    "# Reshape the image to match the input shape expected by the model (1 image, 28x28, 1 channel)\n",
    "image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "\n",
    "# Predict the class of the image using the trained model\n",
    "prediction = model.predict(image)\n",
    "\n",
    "# Print the predicted class (the index of the highest probability in the prediction)\n",
    "print(\"Predicted class:\", np.argmax(prediction))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f73abf7d",
   "metadata": {},
   "source": [
    "Dense layer = fully connected layers used for decision making\n",
    "\n",
    "Conv2D,  #Conv2D layer is responsible for detecting features in the input image\n",
    "\n",
    "Flatten, # converts 2d feature map into 1d vectors\n",
    "\n",
    "MaxPooling2D  #  reduce the spatial dimensions of the feature(height, width)\n",
    "\n",
    "\n",
    "For this practical, the external examiner might ask you questions related to the following topics. Here's a breakdown of important theory that could be useful:\n",
    "\n",
    "### 1. **Convolutional Neural Networks (CNNs)**\n",
    "   - **What is a Convolutional Neural Network (CNN)?**\n",
    "     - A CNN is a type of deep learning model primarily used for image processing tasks. It is designed to automatically and adaptively learn spatial hierarchies of features, making it highly effective for image classification and recognition.\n",
    "   \n",
    "   - **Key Layers in CNN:**\n",
    "     - **Convolutional Layer (`Conv2D`)**: This layer applies convolution operations to the input, using filters (kernels) to detect patterns like edges, textures, and objects. The purpose is to extract features from the input image.\n",
    "     - **Activation Function (`ReLU`)**: Rectified Linear Unit (ReLU) is commonly used as an activation function in CNNs to introduce non-linearity, enabling the model to learn complex patterns.\n",
    "     - **Pooling Layer (`MaxPooling2D`)**: Reduces the spatial dimensions of the feature maps, preserving important features while reducing the number of parameters and computation, which helps prevent overfitting.\n",
    "     - **Flatten Layer**: Converts the 2D feature map into a 1D vector so that it can be input into the fully connected layers.\n",
    "     - **Dense Layer**: Fully connected layers that interpret the features extracted by the convolutional and pooling layers. These layers are used for classification tasks.\n",
    "\n",
    "### 2. **Image Preprocessing**\n",
    "   - **Why do we normalize images?**\n",
    "     - Image pixel values typically range from 0 to 255, but models train more efficiently when values are scaled between 0 and 1. This is done by dividing the pixel values by 255, making the data easier to process and speeding up convergence during training.\n",
    "   \n",
    "   - **Why reshape the images?**\n",
    "     - Neural networks expect input in a specific shape, which for CNNs is typically `(batch_size, height, width, channels)`. For grayscale images like MNIST, we add a channel dimension (1 for grayscale) to the image data to make the shape `(num_samples, 28, 28, 1)`.\n",
    "\n",
    "### 3. **Model Compilation and Training**\n",
    "   - **What is the `compile()` method?**\n",
    "     - The `compile()` method configures the model for training. It defines the optimizer, loss function, and evaluation metrics. \n",
    "     - **Optimizer (`adam`)**: Adam (Adaptive Moment Estimation) is a popular optimizer that computes adaptive learning rates for each parameter by considering both first-order momentum and second-order gradients. It generally performs well on a variety of tasks.\n",
    "     - **Loss Function (`sparse_categorical_crossentropy`)**: This is used for multi-class classification problems where labels are provided as integers (not one-hot encoded). It measures the difference between the true labels and predicted probabilities.\n",
    "     - **Metrics (`accuracy`)**: Accuracy is used to evaluate the performance of the model, representing the percentage of correct predictions.\n",
    "\n",
    "   - **What is `fit()` method?**\n",
    "     - The `fit()` method trains the model on the provided data for a specified number of epochs (iterations over the entire dataset). It adjusts the weights based on the loss function using the specified optimizer.\n",
    "\n",
    "### 4. **Model Evaluation and Prediction**\n",
    "   - **What is the `evaluate()` method?**\n",
    "     - The `evaluate()` method tests the model on unseen data (test set) to assess its performance. It returns the loss value and the metrics defined during compilation (such as accuracy).\n",
    "   \n",
    "   - **What is `predict()` method?**\n",
    "     - The `predict()` method generates predictions for input data. For classification tasks, it outputs probabilities for each class, and the class with the highest probability is typically chosen as the predicted class.\n",
    "\n",
    "### 5. **Overfitting and Regularization**\n",
    "   - **What is overfitting, and how do you prevent it?**\n",
    "     - Overfitting occurs when the model learns the details of the training data too well, including noise and outliers, which results in poor generalization to unseen data. \n",
    "     - **Dropout**: Dropout randomly drops a fraction of neurons during training, preventing the model from becoming overly reliant on specific neurons, thus helping to prevent overfitting.\n",
    "   \n",
    "### 6. **Understanding the MNIST Dataset**\n",
    "   - **What is the MNIST dataset?**\n",
    "     - The MNIST (Modified National Institute of Standards and Technology) dataset consists of 60,000 training images and 10,000 test images of handwritten digits (0-9). Each image is 28x28 pixels in grayscale, making it a popular benchmark for image classification tasks.\n",
    "\n",
    "### 7. **Activation Functions**\n",
    "   - **Why use ReLU (`relu`)?**\n",
    "     - ReLU is widely used in CNNs because it allows models to learn faster and reduces the likelihood of vanishing gradients, which can occur with other activation functions like sigmoid or tanh.\n",
    "   \n",
    "   - **Why use Softmax in the output layer?**\n",
    "     - Softmax is used in the output layer for multi-class classification. It converts the raw output scores (logits) from the network into a probability distribution over the classes, making it easier to interpret the model’s predictions.\n",
    "\n",
    "### 8. **Matplotlib and Visualization**\n",
    "   - **Why use Matplotlib (`plt.imshow`) to visualize images?**\n",
    "     - Visualization helps to understand how the model is processing and interpreting the data. It’s important to visually inspect the dataset to ensure proper preprocessing and to check that the images are being correctly loaded and passed through the network.\n",
    "\n",
    "---\n",
    "\n",
    "### Potential Questions from the Examiner:\n",
    "- What is the purpose of each layer in a CNN (e.g., Conv2D, MaxPooling2D)?\n",
    "- Explain how the convolution operation works.\n",
    "- Why do we use ReLU activation instead of sigmoid or tanh?\n",
    "- What is the difference between `categorical_crossentropy` and `sparse_categorical_crossentropy` loss functions?\n",
    "- Why is dropout used in the network, and how does it prevent overfitting?\n",
    "- How do you evaluate the performance of a classification model?\n",
    "- How does the Adam optimizer work, and why is it preferred over other optimizers?\n",
    "- Explain the role of the `Flatten` layer in a CNN architecture.\n",
    "- What is the significance of normalizing image data before feeding it to the model?\n",
    "\n",
    "Being ready with these concepts and explanations will help you handle related questions from the external examiner.\n",
    "\n",
    "\n",
    "Certainly! Here are the answers to the potential questions that an examiner might ask:\n",
    "\n",
    "### 1. **What is the purpose of each layer in a CNN (e.g., Conv2D, MaxPooling2D)?**\n",
    "   - **Conv2D**: The convolutional layer applies a set of filters (kernels) to the input image to extract features such as edges, textures, or more complex patterns. It performs convolution operations that slide across the image.\n",
    "   - **MaxPooling2D**: This layer performs downsampling by selecting the maximum value from a pool of neighboring pixels (usually in a 2x2 window). It reduces the spatial dimensions of the image (height and width), which helps reduce computation and avoid overfitting.\n",
    "   - **Flatten**: This layer converts the 2D feature map into a 1D vector, which can be passed into fully connected (Dense) layers for classification.\n",
    "   - **Dense**: Fully connected layers where each neuron is connected to every neuron in the previous layer. These layers help the model learn complex combinations of features.\n",
    "   - **Dropout**: This regularization layer randomly sets a fraction of input units to zero during training, helping to prevent overfitting by reducing reliance on specific neurons.\n",
    "\n",
    "### 2. **Explain how the convolution operation works.**\n",
    "   - The convolution operation involves applying a filter (a small matrix, typically 3x3 or 5x5) to the input image. The filter slides across the image and computes the dot product between the filter values and the corresponding input values at each position. This produces a feature map that highlights specific features, such as edges or textures, in the image.\n",
    "\n",
    "### 3. **Why do we use ReLU activation instead of sigmoid or tanh?**\n",
    "   - **ReLU** (Rectified Linear Unit) is preferred because it allows models to learn faster and avoid the vanishing gradient problem. Unlike **sigmoid** and **tanh**, which compress input values into a small range (0-1 for sigmoid, -1 to 1 for tanh), ReLU allows for a wider range of values and accelerates the learning process. It also ensures that the gradient is not zero for positive inputs, which helps with gradient-based optimization during training.\n",
    "\n",
    "### 4. **What is the difference between `categorical_crossentropy` and `sparse_categorical_crossentropy` loss functions?**\n",
    "   - **Categorical Crossentropy**: Used when the target labels are one-hot encoded, meaning the target for each sample is a vector where only one element is 1 (the correct class) and all others are 0.\n",
    "   - **Sparse Categorical Crossentropy**: Used when the target labels are integers, representing the class index (e.g., for 10 classes, the target label for each sample is an integer between 0 and 9). This is useful when the target is not one-hot encoded.\n",
    "\n",
    "### 5. **Why is dropout used in the network, and how does it prevent overfitting?**\n",
    "   - **Dropout** helps to prevent overfitting by randomly setting a fraction of neurons to zero during training. This forces the network to learn redundant representations of the data, ensuring that the model doesn't rely too heavily on any particular neuron and generalizes better to unseen data.\n",
    "\n",
    "### 6. **How do you evaluate the performance of a classification model?**\n",
    "   - The performance of a classification model can be evaluated using metrics such as **accuracy** (percentage of correct predictions), **precision** (accuracy of positive predictions), **recall** (ability to find all positive instances), and **F1-score** (harmonic mean of precision and recall). For multi-class classification, **accuracy** is typically used.\n",
    "\n",
    "### 7. **How does the Adam optimizer work, and why is it preferred over other optimizers?**\n",
    "   - **Adam (Adaptive Moment Estimation)** is an optimization algorithm that computes adaptive learning rates for each parameter by considering both the first moment (mean of gradients) and second moment (uncentered variance of gradients). It combines the advantages of both **Adagrad** and **RMSprop**, allowing it to adapt the learning rate and converge faster. It's preferred because it works well across a variety of problems and doesn't require manual tuning of the learning rate.\n",
    "\n",
    "### 8. **Explain the role of the Flatten layer in a CNN architecture.**\n",
    "   - The **Flatten** layer is used to convert the 2D output from the convolutional and pooling layers into a 1D vector, which can then be fed into fully connected layers (Dense layers). This transformation is necessary because Dense layers expect 1D input.\n",
    "\n",
    "### 9. **What is the significance of normalizing image data before feeding it to the model?**\n",
    "   - **Normalization** rescales the pixel values of the image from the range [0, 255] to [0, 1] by dividing by 255. This ensures that all input features have similar magnitudes, making the optimization process more efficient and speeding up model convergence. Without normalization, large input values can dominate the learning process, slowing down training or causing instability.\n",
    "\n",
    "### 10. **What is the MNIST dataset, and how is it used in this practical?**\n",
    "   - The **MNIST** dataset is a widely used collection of 28x28 grayscale images of handwritten digits (0-9). It is commonly used for benchmarking machine learning models. In this practical, the MNIST dataset is used to train and evaluate a convolutional neural network (CNN) to classify handwritten digits.\n",
    "\n",
    "---\n",
    "\n",
    "Being prepared with both the theory and answers will help you confidently explain your practical during the oral examination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db573d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
